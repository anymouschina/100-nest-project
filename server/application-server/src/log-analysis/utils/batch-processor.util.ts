import { Logger } from '@nestjs/common';

export interface BatchConfig {
  baseBatchSize: number;
  maxBatchSize: number;
  maxConcurrency: number;
  largeDataThreshold: number;
}

export const DEFAULT_CONFIG: BatchConfig = {
  baseBatchSize: 25, // æå‡æ‰¹æ¬¡å¤§å°ä»5åˆ°25
  maxBatchSize: 100, // å¤§æ•°æ®é‡æ—¶æœ€å¤§æ‰¹æ¬¡
  maxConcurrency: 6, // æœ€å¤§å¹¶å‘æ•°
  largeDataThreshold: 500, // å¤§æ•°æ®é‡é˜ˆå€¼
};

export class BatchProcessor {
  private readonly logger = new Logger(BatchProcessor.name);

  constructor(private readonly config: BatchConfig = DEFAULT_CONFIG) {}

  // ğŸ”¥ æ™ºèƒ½æ‰¹æ¬¡åˆ‡ç‰‡
  createBatches<T>(data: T[]): T[][] {
    if (data.length === 0) return [];

    const batchSize = this.calculateBatchSize(data.length);
    const batches: T[][] = [];

    for (let i = 0; i < data.length; i += batchSize) {
      batches.push(data.slice(i, i + batchSize));
    }

    this.logger.debug(
      `åˆ›å»ºæ‰¹æ¬¡: ${data.length}æ¡æ•°æ® -> ${batches.length}ä¸ªæ‰¹æ¬¡ (æ¯æ‰¹${batchSize}æ¡)`,
    );

    return batches;
  }

  // ğŸ”¥ å¹¶è¡Œæ‰¹æ¬¡å¤„ç†
  async processInParallel<T, R>(
    batches: T[][],
    processor: (batch: T[], index: number) => Promise<R[]>,
  ): Promise<R[]> {
    const results: R[] = [];
    const concurrency = Math.min(batches.length, this.config.maxConcurrency);

    this.logger.debug(
      `å¹¶è¡Œå¤„ç†${batches.length}ä¸ªæ‰¹æ¬¡ï¼Œå¹¶å‘æ•°ï¼š${concurrency}`,
    );

    // åˆ†ç»„å¹¶è¡Œå¤„ç†
    for (let i = 0; i < batches.length; i += concurrency) {
      const batchGroup = batches.slice(i, i + concurrency);

      const groupPromises = batchGroup.map((batch, index) =>
        processor(batch, i + index),
      );

      const groupResults = await Promise.all(groupPromises);
      results.push(...groupResults.flat());

      this.logger.debug(
        `å®Œæˆæ‰¹æ¬¡ç»„ ${Math.floor(i / concurrency) + 1}/${Math.ceil(batches.length / concurrency)}`,
      );
    }

    return results;
  }

  // ğŸ”¥ æ™ºèƒ½æ—¥å¿—åˆ†å±‚
  stratifyLogs(logs: any[]): { tier1: any[]; tier2: any[]; tier3: any[] } {
    const tier1: any[] = []; // é”™è¯¯å’Œå¼‚å¸¸
    const tier2: any[] = []; // è­¦å‘Šå’Œé‡è¦ä¿¡æ¯
    const tier3: any[] = []; // ä¸€èˆ¬ä¿¡æ¯

    logs.forEach((log) => {
      const logStr = typeof log === 'string' ? log.toLowerCase() : JSON.stringify(log).toLowerCase();

      if (
        logStr.includes('error') ||
        logStr.includes('exception') ||
        logStr.includes('fail') ||
        logStr.includes('crash')
      ) {
        tier1.push(log);
      } else if (
        logStr.includes('warn') ||
        logStr.includes('user') ||
        logStr.includes('login') ||
        logStr.includes('payment')
      ) {
        tier2.push(log);
      } else {
        tier3.push(log);
      }
    });

    // æ™ºèƒ½é‡‡æ ·ç¬¬ä¸‰å±‚çº§
    if (tier3.length > 300) {
      tier3.splice(300); // ä¿ç•™å‰300æ¡
    }

    return { tier1, tier2, tier3 };
  }

  // ğŸ”¥ è·å–å¤„ç†ç»Ÿè®¡
  getProcessingStats(totalLogs: number): {
    strategy: string;
    batchSize: number;
    batches: number;
    concurrency: number;
    estimatedTime: string;
  } {
    const batchSize = this.calculateBatchSize(totalLogs);
    const batches = Math.ceil(totalLogs / batchSize);
    const concurrency = Math.min(batches, this.config.maxConcurrency);
    const estimatedSeconds = Math.ceil(batches / concurrency * 2);

    return {
      strategy: totalLogs >= this.config.largeDataThreshold ? 'stratified' : 'parallel',
      batchSize,
      batches,
      concurrency,
      estimatedTime: `${estimatedSeconds}ç§’`,
    };
  }

  private calculateBatchSize(totalLogs: number): number {
    if (totalLogs <= 50) {
      return Math.max(5, Math.floor(totalLogs / 2));
    } else if (totalLogs <= 200) {
      return this.config.baseBatchSize;
    } else if (totalLogs >= this.config.largeDataThreshold) {
      return this.config.maxBatchSize;
    } else {
      return Math.floor(this.config.baseBatchSize * 1.5);
    }
  }
} 