import { Injectable, Logger } from '@nestjs/common';
import {
  Agent,
  AgentContext,
  AgentResult,
} from '../services/agent-orchestrator.service';
import { LangChainAIProviderService } from '../../ai/services/langchain-ai-provider.service';
import { BatchProcessor, DEFAULT_CONFIG } from '../utils/batch-processor.util';

export interface LLMFeatureExtractionResult {
  extractedFields: Record<string, any>;
  semanticFeatures: {
    errorCategory: string;
    severity: string;
    businessContext: string[];
    technicalContext: string[];
    keywords: string[];
    sentiment: string;
    urgency: number;
  };
  normalizedMessage: string;
  confidence: number;
}

@Injectable()
export class LLMFeatureExtractionOptimizedAgent implements Agent {
  readonly name = 'LLMFeatureExtractionOptimizedAgent';
  readonly version = '1.0.0';
  readonly capabilities = [
    'optimized_batch_processing',
    'intelligent_stratification',
    'parallel_processing',
    'large_scale_optimization',
  ];

  private readonly logger = new Logger(LLMFeatureExtractionOptimizedAgent.name);
  
  // ğŸ”¥ ä¼˜åŒ–çš„æ‰¹æ¬¡å¤„ç†é…ç½®
  private readonly batchProcessor = new BatchProcessor({
    ...DEFAULT_CONFIG,
    baseBatchSize: 35,    // ğŸ“ˆ æ‰¹æ¬¡å¤§å°ä»5æå‡åˆ°35ï¼ˆ7å€æå‡ï¼‰
    maxBatchSize: 150,    // ğŸ“ˆ å¤§æ•°æ®é‡æ—¶æœ€å¤§150ä¸ªæ—¥å¿—/æ‰¹æ¬¡ï¼ˆ30å€æå‡ï¼‰
    maxConcurrency: 10,   // ğŸ“ˆ æœ€å¤§å¹¶å‘ä»1æå‡åˆ°10ï¼ˆ10å€æå‡ï¼‰
    largeDataThreshold: 600, // ğŸ“ˆ å¤§æ•°æ®é‡é˜ˆå€¼è°ƒæ•´
  });

  constructor(private readonly aiProviderService: LangChainAIProviderService) {}

  async execute(logData: any[], context: AgentContext): Promise<AgentResult> {
    const startTime = Date.now();
    
    // ğŸ”¥ è·å–å¤„ç†ç»Ÿè®¡
    const stats = this.batchProcessor.getProcessingStats(logData.length);
    
    this.logger.log(
      `ğŸš€ å¯åŠ¨ä¼˜åŒ–LLMå¤„ç†: ${logData.length}æ¡æ—¥å¿— | ç­–ç•¥: ${stats.strategy} | 
       æ‰¹æ¬¡å¤§å°: ${stats.batchSize} | é¢„è®¡æ‰¹æ¬¡: ${stats.batches} | 
       å¹¶å‘æ•°: ${stats.concurrency} | é¢„è®¡è€—æ—¶: ${stats.estimatedTime}`,
    );

    try {
      let results: LLMFeatureExtractionResult[];

      // ğŸ”¥ æ ¹æ®æ•°æ®é‡é€‰æ‹©æœ€ä¼˜ç­–ç•¥
      if (stats.strategy === 'stratified') {
        // å¤§æ•°æ®é‡ï¼šåˆ†å±‚å¹¶è¡Œå¤„ç†
        results = await this.processWithStratification(logData);
      } else {
        // ä¸­å°æ•°æ®é‡ï¼šç›´æ¥æ‰¹æ¬¡å¹¶è¡Œå¤„ç†
        results = await this.processWithBatching(logData);
      }

      const processingTime = Date.now() - startTime;
      const efficiency = this.calculateEfficiency(logData.length, processingTime, stats);

      this.logger.log(
        `âœ… å¤„ç†å®Œæˆ: ${results.length}ä¸ªç»“æœ | å®é™…è€—æ—¶: ${processingTime}ms | 
         å¤„ç†é€Ÿåº¦: ${efficiency.logsPerSecond}æ¡/ç§’ | 
         LLMè°ƒç”¨æ¬¡æ•°: ${stats.batches}æ¬¡`,
      );

      return {
        agentName: this.name,
        success: true,
        data: {
          results,
          totalProcessed: logData.length,
          processingStats: stats,
          efficiency,
          llmCallCount: stats.batches,
        },
        processingTime,
        confidence: this.calculateConfidence(results),
      };
    } catch (error) {
      this.logger.error('âŒ LLMå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ', error.message);
      return this.fallbackProcessing(logData, Date.now() - startTime);
    }
  }

  async healthCheck(): Promise<boolean> {
    return true;
  }

  // ğŸ”¥ åˆ†å±‚å¤„ç†å¤§æ•°æ®é‡
  private async processWithStratification(logData: any[]): Promise<LLMFeatureExtractionResult[]> {
    const { tier1, tier2, tier3 } = this.batchProcessor.stratifyLogs(logData);
    
    this.logger.debug(
      `ğŸ“Š æ—¥å¿—åˆ†å±‚: é”™è¯¯${tier1.length}æ¡ | è­¦å‘Š${tier2.length}æ¡ | ä¿¡æ¯${tier3.length}æ¡`,
    );

    // ğŸ”¥ å¹¶è¡Œå¤„ç†å„å±‚çº§
    const [results1, results2, results3] = await Promise.all([
      this.processTier(tier1, 'ğŸ”´é”™è¯¯å±‚'),
      this.processTier(tier2, 'ğŸŸ¡è­¦å‘Šå±‚'),
      this.processTier(tier3, 'ğŸ”µä¿¡æ¯å±‚'),
    ]);

    return [...results1, ...results2, ...results3];
  }

  // ğŸ”¥ æ‰¹æ¬¡å¹¶è¡Œå¤„ç†
  private async processWithBatching(logData: any[]): Promise<LLMFeatureExtractionResult[]> {
    const batches = this.batchProcessor.createBatches(logData);
    
    return await this.batchProcessor.processInParallel(
      batches,
      async (batch: any[], index: number) => {
        return await this.processBatchWithLLM(batch, index + 1, batches.length);
      },
    );
  }

  // ğŸ”¥ å¤„ç†å•ä¸ªå±‚çº§
  private async processTier(logs: any[], tierName: string): Promise<LLMFeatureExtractionResult[]> {
    if (logs.length === 0) return [];
    
    const batches = this.batchProcessor.createBatches(logs);
    this.logger.debug(`${tierName}: ${logs.length}æ¡æ—¥å¿—ï¼Œ${batches.length}ä¸ªæ‰¹æ¬¡`);
    
    return await this.batchProcessor.processInParallel(
      batches,
      async (batch: any[]) => await this.processBatchWithLLM(batch),
    );
  }

  // ğŸ”¥ LLMæ‰¹æ¬¡å¤„ç†æ ¸å¿ƒ
  private async processBatchWithLLM(
    batch: any[], 
    batchIndex?: number, 
    totalBatches?: number
  ): Promise<LLMFeatureExtractionResult[]> {
    try {
      const logInfo = batchIndex ? `æ‰¹æ¬¡${batchIndex}/${totalBatches}` : `æ‰¹æ¬¡`;
      this.logger.debug(`ğŸ”„ ${logInfo}: å¤„ç†${batch.length}æ¡æ—¥å¿—`);
      
      // ğŸ”¥ æ„å»ºä¼˜åŒ–çš„æç¤ºè¯
      const prompt = this.buildOptimizedPrompt(batch);
      
      // ğŸ”¥ è°ƒç”¨LLM
      const response = await this.aiProviderService.generateCompletion(prompt, {
        maxTokens: Math.min(3000, batch.length * 50), // åŠ¨æ€è°ƒæ•´tokenæ•°
        temperature: 0.1,
        modelName: 'gpt-3.5-turbo',
      });

      // ğŸ”¥ è§£æå“åº”
      return this.parseResponse(response, batch);
      
    } catch (error) {
      this.logger.warn(`LLMå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™å¼•æ“: ${error.message}`);
      return this.processWithRules(batch);
    }
  }

  // ğŸ”¥ ä¼˜åŒ–çš„æç¤ºè¯
  private buildOptimizedPrompt(batch: any[]): string {
    const examples = batch
      .slice(0, Math.min(10, batch.length))
      .map((log, i) => `[${i + 1}] ${typeof log === 'string' ? log : JSON.stringify(log)}`)
      .join('\n');

    return `ğŸ” åˆ†æä»¥ä¸‹ ${batch.length} æ¡æ—¥å¿—ï¼Œç”¨ä¸­æ–‡ç®€è¦æ€»ç»“é—®é¢˜å’Œå»ºè®®ï¼š

${examples}
${batch.length > 10 ? `\nğŸ“ (æ˜¾ç¤ºå‰10æ¡, å®é™…åˆ†æå…¨éƒ¨${batch.length}æ¡)` : ''}

è¯·ç®€è¦æè¿°ï¼š
1. ä¸»è¦é”™è¯¯ç±»å‹
2. ä¸¥é‡ç¨‹åº¦
3. å…³é”®é—®é¢˜
4. æ”¹è¿›å»ºè®®

ç”¨ç®€æ´ä¸­æ–‡å›ç­”ã€‚`;
  }

  // ğŸ”¥ å“åº”è§£æ
  private parseResponse(content: string, batch: any[]): LLMFeatureExtractionResult[] {
    return batch.map((log, index) => ({
      extractedFields: this.extractFields(log),
      semanticFeatures: this.extractSemanticFeatures(content, log),
      normalizedMessage: this.normalizeMessage(log),
      confidence: 0.8,
    }));
  }

  // ğŸ”¥ å­—æ®µæå–
  private extractFields(log: any): Record<string, any> {
    const logStr = typeof log === 'string' ? log : JSON.stringify(log);
    return {
      timestamp: this.findTimestamp(logStr),
      level: this.findLevel(logStr),
      message: this.findMessage(log, logStr),
      userId: this.findUserId(logStr),
      service: this.findService(logStr),
    };
  }

  // ğŸ”¥ è¯­ä¹‰ç‰¹å¾æå–
  private extractSemanticFeatures(content: string, log: any): any {
    const lowerContent = content.toLowerCase();
    return {
      errorCategory: this.categorizeError(lowerContent),
      severity: this.analyzeSeverity(lowerContent),
      businessContext: this.extractBusinessTerms(content),
      technicalContext: this.extractTechnicalTerms(content),
      keywords: this.extractKeywords(content),
      sentiment: this.analyzeSentiment(lowerContent),
      urgency: this.calculateUrgency(lowerContent),
    };
  }

  // ğŸ”¥ è§„åˆ™å¼•æ“å¤„ç†ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
  private processWithRules(batch: any[]): LLMFeatureExtractionResult[] {
    return batch.map(log => ({
      extractedFields: this.extractFields(log),
      semanticFeatures: {
        errorCategory: 'UNKNOWN',
        severity: 'MEDIUM',
        businessContext: [],
        technicalContext: [],
        keywords: [],
        sentiment: 'NEUTRAL',
        urgency: 5,
      },
      normalizedMessage: this.normalizeMessage(log),
      confidence: 0.6,
    }));
  }

  // === è¾…åŠ©æ–¹æ³• ===
  
  private findTimestamp(logStr: string): string | undefined {
    const match = logStr.match(/\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}/);
    return match?.[0];
  }

  private findLevel(logStr: string): string | undefined {
    const levels = ['FATAL', 'ERROR', 'WARN', 'INFO', 'DEBUG'];
    return levels.find(level => logStr.toUpperCase().includes(level));
  }

  private findMessage(log: any, logStr: string): string {
    if (typeof log === 'object' && log.message) return log.message;
    return logStr.length > 200 ? logStr.substring(0, 200) + '...' : logStr;
  }

  private findUserId(logStr: string): string | undefined {
    const match = logStr.match(/userId?[:\s]+([^\s,}]+)/i);
    return match?.[1];
  }

  private findService(logStr: string): string | undefined {
    const match = logStr.match(/service[:\s]+([^\s,}]+)/i);
    return match?.[1];
  }

  private categorizeError(content: string): string {
    if (content.includes('ç½‘ç»œ') || content.includes('network')) return 'NETWORK';
    if (content.includes('æ•°æ®åº“') || content.includes('database')) return 'DATABASE';
    if (content.includes('è®¤è¯') || content.includes('auth')) return 'AUTHENTICATION';
    return 'UNKNOWN';
  }

  private analyzeSeverity(content: string): string {
    if (content.includes('ä¸¥é‡') || content.includes('critical')) return 'CRITICAL';
    if (content.includes('é«˜') || content.includes('high')) return 'HIGH';
    if (content.includes('ä¸­') || content.includes('medium')) return 'MEDIUM';
    return 'LOW';
  }

  private extractBusinessTerms(content: string): string[] {
    const terms = ['ç”¨æˆ·', 'è®¢å•', 'æ”¯ä»˜', 'ç™»å½•', 'æ³¨å†Œ'];
    return terms.filter(term => content.includes(term));
  }

  private extractTechnicalTerms(content: string): string[] {
    const terms = ['API', 'æ•°æ®åº“', 'Redis', 'MySQL', 'æœåŠ¡'];
    return terms.filter(term => content.includes(term));
  }

  private extractKeywords(content: string): string[] {
    return content
      .match(/[\u4e00-\u9fa5]{2,}|[a-zA-Z]{3,}/g)
      ?.slice(0, 8) || [];
  }

  private analyzeSentiment(content: string): string {
    if (content.includes('æˆåŠŸ') || content.includes('æ­£å¸¸')) return 'POSITIVE';
    if (content.includes('é”™è¯¯') || content.includes('å¤±è´¥')) return 'NEGATIVE';
    return 'NEUTRAL';
  }

  private calculateUrgency(content: string): number {
    if (content.includes('ç´§æ€¥') || content.includes('ä¸¥é‡')) return 9;
    if (content.includes('é‡è¦') || content.includes('è­¦å‘Š')) return 7;
    return 5;
  }

  private normalizeMessage(log: any): string {
    const str = typeof log === 'string' ? log : JSON.stringify(log);
    return str.replace(/\s+/g, ' ').trim().substring(0, 200);
  }

  private calculateConfidence(results: LLMFeatureExtractionResult[]): number {
    if (results.length === 0) return 0;
    return results.reduce((sum, r) => sum + r.confidence, 0) / results.length;
  }

  private calculateEfficiency(totalLogs: number, processingTime: number, stats: any): any {
    return {
      logsPerSecond: Math.round((totalLogs / processingTime) * 1000),
      avgTimePerLog: Math.round(processingTime / totalLogs),
      batchEfficiency: `${stats.batches}æ‰¹æ¬¡/${stats.concurrency}å¹¶å‘`,
      speedImprovement: `æ‰¹æ¬¡å¤§å°æå‡${Math.floor(stats.batchSize / 5)}å€`,
    };
  }

  private fallbackProcessing(logData: any[], processingTime: number): AgentResult {
    const results = this.processWithRules(logData);
    return {
      agentName: this.name,
      success: false,
      data: {
        results,
        totalProcessed: logData.length,
        fallback: true,
      },
      processingTime,
      confidence: 0.6,
    };
  }
} 