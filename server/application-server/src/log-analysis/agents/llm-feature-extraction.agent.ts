import { Injectable, Logger } from '@nestjs/common';
import {
  Agent,
  AgentContext,
  AgentResult,
} from '../services/agent-orchestrator.service';
import { LangChainAIProviderService } from '../../ai/services/langchain-ai-provider.service';
import { BatchProcessor, DEFAULT_CONFIG } from '../utils/batch-processor.util';

export interface LLMFeatureExtractionResult {
  extractedFields: {
    timestamp?: string;
    level?: string;
    message?: string;
    source?: string;
    service?: string;
    userId?: string;
    sessionId?: string;
    responseTime?: number;
    errorCode?: string;
    [key: string]: any;
  };
  semanticFeatures: {
    errorCategory:
      | 'NETWORK'
      | 'DATABASE'
      | 'AUTHENTICATION'
      | 'BUSINESS_LOGIC'
      | 'SYSTEM'
      | 'UNKNOWN';
    severity: 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL';
    businessContext: string[];
    technicalContext: string[];
    keywords: string[];
    sentiment: 'POSITIVE' | 'NEGATIVE' | 'NEUTRAL';
    urgency: number;
  };
  normalizedMessage: string;
  confidence: number;
}

// ğŸ”¥ æ–°å¢ï¼šæ‰¹æ¬¡å¤„ç†é…ç½®æ¥å£
export interface BatchProcessingConfig {
  baseBatchSize: number;
  maxBatchSize: number;
  minBatchSize: number;
  maxConcurrency: number;
  intelligentSampling: boolean;
  prioritizedProcessing: boolean;
}

// ğŸ”¥ æ–°å¢ï¼šæ—¥å¿—åˆ†å±‚æ¥å£
export interface LogTier {
  priority: number;
  logs: any[];
  batchSize: number;
  description: string;
}

@Injectable()
export class LLMFeatureExtractionAgent implements Agent {
  readonly name = 'LLMFeatureExtractionAgent';
  readonly version = '3.0.0'; // å‡çº§ç‰ˆæœ¬å·
  readonly capabilities = [
    'adaptive_field_extraction',
    'semantic_understanding',
    'format_normalization',
    'intelligent_categorization',
    'context_awareness',
    'dynamic_batch_processing', // ğŸ”¥ æ–°å¢èƒ½åŠ›
    'parallel_processing',      // ğŸ”¥ æ–°å¢èƒ½åŠ›
    'large_scale_optimization', // ğŸ”¥ æ–°å¢èƒ½åŠ›
  ];

  private readonly logger = new Logger(LLMFeatureExtractionAgent.name);
  private readonly batchProcessor = new BatchProcessor(DEFAULT_CONFIG);
  // æ·»åŠ æ‰¹æ¬¡é…ç½®å±æ€§
  private readonly batchConfig: BatchProcessingConfig = {
    baseBatchSize: 35,
    maxBatchSize: 150,
    minBatchSize: 10,
    maxConcurrency: 10,
    intelligentSampling: true,
    prioritizedProcessing: true,
  };

  constructor(private readonly aiProviderService: LangChainAIProviderService) {}

  async execute(logData: any[], context: AgentContext): Promise<AgentResult> {
    const startTime = Date.now();
    this.logger.debug(
      `ğŸ”¥ å¼€å§‹ä¼˜åŒ–LLMç‰¹å¾æå–: ${context.taskId}, æ—¥å¿—æ•°é‡: ${logData.length}`,
    );

    try {
      // ğŸ”¥ è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯
      const stats = this.batchProcessor.getProcessingStats(logData.length);
      this.logger.log(
        `å¤„ç†ç­–ç•¥: ${stats.strategy}, æ‰¹æ¬¡å¤§å°: ${stats.batchSize}, 
         é¢„è®¡æ‰¹æ¬¡æ•°: ${stats.batches}, å¹¶å‘æ•°: ${stats.concurrency}, 
         é¢„è®¡æ—¶é—´: ${stats.estimatedTime}`,
      );

      let results: LLMFeatureExtractionResult[] = [];

      // ğŸ”¥ æ ¹æ®æ•°æ®é‡é€‰æ‹©æœ€ä¼˜å¤„ç†ç­–ç•¥
      if (stats.strategy === 'stratified') {
        // å¤§æ•°æ®é‡: åˆ†å±‚å¤„ç†
        results = await this.processWithStratification(logData);
      } else {
        // ä¸­å°æ•°æ®é‡: å¹¶è¡Œæ‰¹æ¬¡å¤„ç†
        results = await this.processWithBatching(logData);
      }

      const aggregatedFeatures = this.aggregateFeatures(results);
      const processingTime = Date.now() - startTime;

      this.logger.log(
        `âœ… LLMç‰¹å¾æå–å®Œæˆ: å¤„ç†${logData.length}æ¡æ—¥å¿—, 
         å®é™…è€—æ—¶${processingTime}ms, ç”Ÿæˆ${results.length}ä¸ªç»“æœ`,
      );

      return {
        agentName: this.name,
        success: true,
        data: {
          individualResults: results,
          aggregatedFeatures,
          totalProcessed: logData.length,
          llmEnabled: true,
          processingStats: stats,
          actualProcessingTime: processingTime,
        },
        processingTime,
        confidence: this.calculateOverallConfidence(results),
      };
    } catch (error) {
      this.logger.error('LLMç‰¹å¾æå–å¤±è´¥', error.stack);
      return {
        agentName: this.name,
        success: false,
        data: await this.fallbackExtraction(logData),
        processingTime: Date.now() - startTime,
        confidence: 0.6,
      };
    }
  }

  async healthCheck(): Promise<boolean> {
    return true;
  }

  // ğŸ”¥ æ–°å¢ï¼šåˆ†å±‚å¤„ç†æ–¹æ³•
  private async processWithStratification(logData: any[]): Promise<LLMFeatureExtractionResult[]> {
    const { tier1, tier2, tier3 } = this.batchProcessor.stratifyLogs(logData);
    const results: LLMFeatureExtractionResult[] = [];

    // å¹¶è¡Œå¤„ç†å„å±‚çº§
    const tierPromises = [
      this.processLogTier(tier1, 'å…³é”®é”™è¯¯å±‚'),
      this.processLogTier(tier2, 'è­¦å‘Šä¿¡æ¯å±‚'),
      this.processLogTier(tier3, 'ä¸€èˆ¬ä¿¡æ¯å±‚'),
    ];

    const tierResults = await Promise.all(tierPromises);
    return tierResults.flat();
  }

  // ğŸ”¥ æ–°å¢ï¼šæ‰¹æ¬¡å¤„ç†æ–¹æ³•
  private async processWithBatching(logData: any[]): Promise<LLMFeatureExtractionResult[]> {
    const batches = this.batchProcessor.createBatches(logData);
    
    return await this.batchProcessor.processInParallel(
      batches,
      async (batch: any[], index: number) => {
        this.logger.debug(`å¤„ç†æ‰¹æ¬¡ ${index + 1}/${batches.length}`);
        return await this.processBatch(batch);
      },
    );
  }

  // ğŸ”¥ æ–°å¢ï¼šå±‚çº§å¤„ç†æ–¹æ³•
  private async processLogTier(logs: any[], tierName: string): Promise<LLMFeatureExtractionResult[]> {
    if (logs.length === 0) return [];
    
    this.logger.debug(`å¤„ç†${tierName}: ${logs.length}æ¡æ—¥å¿—`);
    const batches = this.batchProcessor.createBatches(logs);
    
    return await this.batchProcessor.processInParallel(
      batches,
      async (batch: any[]) => await this.processBatch(batch),
    );
  }

  // ğŸ”¥ ä¼˜åŒ–LLMæç¤ºè¯ç”Ÿæˆ
  private buildLLMPrompt(logEntries: any[]): string {
    const maxExamples = Math.min(logEntries.length, 8); 
    const examples = logEntries
      .slice(0, maxExamples)
      .map((log, index) => {
        const logStr = typeof log === 'string' ? log : JSON.stringify(log);
        return `[${index + 1}] ${logStr}`;
      })
      .join('\n');

    const batchInfo =
      logEntries.length > maxExamples
        ? `\n\nğŸ“ æ³¨æ„: æ­¤æ‰¹æ¬¡åŒ…å« ${logEntries.length} æ¡æ—¥å¿—ï¼Œä¸Šé¢ä»…æ˜¾ç¤ºå‰ ${maxExamples} æ¡ä½œä¸ºç¤ºä¾‹ï¼Œè¯·åŸºäºå…¨éƒ¨æ—¥å¿—è¿›è¡Œåˆ†æã€‚`
        : '';

    return `ğŸ” è¯·åˆ†æä»¥ä¸‹ ${logEntries.length} æ¡æ—¥å¿—ï¼Œç”¨ä¸­æ–‡ç®€è¦æè¿°å‘ç°çš„é—®é¢˜å’Œå»ºè®®ï¼š

${examples}${batchInfo}

ğŸ“‹ é‡ç‚¹åˆ†æå†…å®¹ï¼š
1. ğŸ·ï¸ é”™è¯¯ç±»åˆ«è¯†åˆ«ï¼ˆç½‘ç»œ/æ•°æ®åº“/è®¤è¯/ä¸šåŠ¡é€»è¾‘/ç³»ç»Ÿï¼‰
2. ğŸš¨ ä¸¥é‡ç¨‹åº¦è¯„ä¼°ï¼ˆä½/ä¸­/é«˜/ä¸¥é‡ï¼‰
3. ğŸ‘¤ ç”¨æˆ·è¡Œä¸ºæ¨¡å¼åˆ†æ
4. âš™ï¸ æŠ€æœ¯é—®é¢˜å®šä½
5. ğŸ˜Š æ•´ä½“æƒ…æ„Ÿå€¾å‘
6. â° ç´§æ€¥ç¨‹åº¦åˆ¤æ–­

ğŸ¯ è¯·ç”¨ç®€æ´çš„ä¸­æ–‡æè¿°æ‚¨çš„å‘ç°å’Œæ”¹è¿›å»ºè®®ã€‚`;
  }

  // ğŸ”¥ æ–°å¢ï¼šæ™ºèƒ½é‡‡æ ·
  private intelligentSample(logs: any[], targetSize: number): any[] {
    if (logs.length <= targetSize) return logs;

    const sampled: any[] = [];
    const step = Math.floor(logs.length / targetSize);
    
    // å‡åŒ€é‡‡æ · + å…³é”®æ—¥å¿—ä¿ç•™
    for (let i = 0; i < logs.length; i += step) {
      sampled.push(logs[i]);
      if (sampled.length >= targetSize) break;
    }

    // ç¡®ä¿åŒ…å«ä¸€äº›å…³é”®æ—¥å¿—
    const criticalLogs = logs.filter(log => {
      const logStr = typeof log === 'string' ? log : JSON.stringify(log);
      return logStr.toLowerCase().includes('error') || 
             logStr.toLowerCase().includes('fail') || 
             logStr.toLowerCase().includes('timeout');
    });

    // æ›¿æ¢ä¸€äº›æ™®é€šæ—¥å¿—ä¸ºå…³é”®æ—¥å¿—
    const replacementCount = Math.min(criticalLogs.length, Math.floor(targetSize * 0.3));
    for (let i = 0; i < replacementCount; i++) {
      if (i < sampled.length) {
        sampled[i] = criticalLogs[i];
      }
    }

    return sampled.slice(0, targetSize);
  }

  // ğŸ”¥ æ–°å¢ï¼šå¹¶è¡Œæ‰¹æ¬¡å¤„ç†
  private async processWithParallelBatches(logData: any[]): Promise<LLMFeatureExtractionResult[]> {
    const optimalBatchSize = this.calculateOptimalBatchSize(logData.length);
    const batches = this.createDynamicBatches(logData, optimalBatchSize);
    
    return await this.processParallelBatches(batches, 'ParallelProcessing');
  }

  // ğŸ”¥ æ–°å¢ï¼šä¼˜åŒ–çš„æ‰¹æ¬¡å¤„ç†
  private async processWithOptimizedBatches(logData: any[]): Promise<LLMFeatureExtractionResult[]> {
    const batchSize = Math.max(
      this.batchConfig.minBatchSize,
      Math.min(this.batchConfig.baseBatchSize, logData.length)
    );
    
    const batches = this.createDynamicBatches(logData, batchSize);
    const results: LLMFeatureExtractionResult[] = [];

    for (const batch of batches) {
      const batchResults = await this.processBatch(batch);
      results.push(...batchResults);
    }

    return results;
  }

  // ğŸ”¥ æ–°å¢ï¼šå¹¶è¡Œæ‰¹æ¬¡æ‰§è¡Œå™¨
  private async processParallelBatches(
    batches: any[][],
    context: string
  ): Promise<LLMFeatureExtractionResult[]> {
    const results: LLMFeatureExtractionResult[] = [];
    
    // æ§åˆ¶å¹¶å‘æ•°é‡
    for (let i = 0; i < batches.length; i += this.batchConfig.maxConcurrency) {
      const batchGroup = batches.slice(i, i + this.batchConfig.maxConcurrency);
      
      this.logger.debug(`${context}: å¹¶è¡Œå¤„ç†æ‰¹æ¬¡ç»„ ${Math.floor(i / this.batchConfig.maxConcurrency) + 1}/${Math.ceil(batches.length / this.batchConfig.maxConcurrency)}`);
      
      const groupPromises = batchGroup.map(async (batch, index) => {
        try {
          return await this.processBatch(batch);
        } catch (error) {
          this.logger.warn(`æ‰¹æ¬¡ ${i + index + 1} å¤„ç†å¤±è´¥: ${error.message}`);
          return this.processWithRules(batch); // é™çº§å¤„ç†
        }
      });

      const groupResults = await Promise.all(groupPromises);
      results.push(...groupResults.flat());
    }

    return results;
  }

  // ğŸ”¥ æ–°å¢ï¼šåŠ¨æ€æ‰¹æ¬¡å¤§å°è®¡ç®—
  private calculateOptimalBatchSize(totalLogs: number): number {
    if (totalLogs < 50) {
      return this.batchConfig.minBatchSize;
    } else if (totalLogs < 200) {
      return this.batchConfig.baseBatchSize;
    } else if (totalLogs < 1000) {
      return this.batchConfig.baseBatchSize * 2;
    } else {
      return this.batchConfig.maxBatchSize;
    }
  }

  // ğŸ”¥ æ–°å¢ï¼šåŠ¨æ€æ‰¹æ¬¡åˆ›å»º
  private createDynamicBatches<T>(array: T[], batchSize: number): T[][] {
    const batches: T[][] = [];
    for (let i = 0; i < array.length; i += batchSize) {
      batches.push(array.slice(i, i + batchSize));
    }
    return batches;
  }

  // ğŸ”¥ æ–°å¢ï¼šè¾…åŠ©æ–¹æ³•
  private extractLogLevel(log: any, logStr: string): string {
    return this.smartExtractLevel(log, logStr) || 'INFO';
  }

  private isImportantInfo(logStr: string): boolean {
    const importantKeywords = [
      'user', 'login', 'logout', 'payment', 'order', 
      'critical', 'important', 'significant', 'alert'
    ];
    const lowerStr = logStr.toLowerCase();
    return importantKeywords.some(keyword => lowerStr.includes(keyword));
  }

  private getProcessingStrategy(logCount: number): string {
    if (logCount > 1000) return 'large_dataset_stratified';
    if (logCount > 100) return 'medium_dataset_parallel';
    return 'small_dataset_standard';
  }

  private getBatchingStats(logCount: number): any {
    const strategy = this.getProcessingStrategy(logCount);
    const optimalBatchSize = this.calculateOptimalBatchSize(logCount);
    const estimatedBatches = Math.ceil(logCount / optimalBatchSize);
    
    return {
      strategy,
      optimalBatchSize,
      estimatedBatches,
      maxConcurrency: this.batchConfig.maxConcurrency,
      estimatedLLMCalls: estimatedBatches,
      estimatedProcessingTime: `${Math.ceil(estimatedBatches / this.batchConfig.maxConcurrency * 2)}ç§’`,
    };
  }

  // æ‰¹é‡å¤„ç†æ—¥å¿—
  private async processBatch(
    logEntries: any[],
  ): Promise<LLMFeatureExtractionResult[]> {
    try {
      return await this.processWithLLM(logEntries);
    } catch (error) {
      this.logger.warn(`LLMå¤„ç†å¤±è´¥ï¼Œé™çº§åˆ°è§„åˆ™å¼•æ“: ${error.message}`);
      return this.processWithRules(logEntries);
    }
  }

  // ä½¿ç”¨LLMå¤„ç†
  private async processWithLLM(
    logEntries: any[],
  ): Promise<LLMFeatureExtractionResult[]> {
    const prompt = this.buildLLMPrompt(logEntries);

    const systemPrompt = `ä½ æ˜¯ä¸“ä¸šçš„æ—¥å¿—åˆ†æä¸“å®¶ã€‚è¯·ç”¨ä¸­æ–‡åˆ†ææ—¥å¿—å†…å®¹ï¼Œé‡ç‚¹å…³æ³¨ï¼š

1. é”™è¯¯æ¨¡å¼è¯†åˆ« - ç½‘ç»œã€æ•°æ®åº“ã€è®¤è¯ã€ä¸šåŠ¡é€»è¾‘ã€ç³»ç»Ÿç±»å‹
2. ä¸¥é‡ç¨‹åº¦è¯„ä¼° - æ ¹æ®å½±å“èŒƒå›´å’Œç´§æ€¥ç¨‹åº¦åˆ¤æ–­
3. ç”¨æˆ·è¡Œä¸ºåˆ†æ - è¯†åˆ«å¼‚å¸¸æ“ä½œæ¨¡å¼å’Œé‡å¤è¡Œä¸º
4. æŠ€æœ¯æŒ‡æ ‡è¯„ä¼° - å“åº”æ—¶é—´ã€é”™è¯¯ç‡ã€ç³»ç»ŸçŠ¶æ€
5. æƒ…æ„Ÿå€¾å‘åˆ†æ - æ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§

è¯·ç”¨è‡ªç„¶è¯­è¨€æè¿°ä½ çš„å‘ç°ï¼ŒåŒ…æ‹¬é—®é¢˜æè¿°ã€åŸå› åˆ†æå’Œå»ºè®®æªæ–½ã€‚`;

    try {
      // ä½¿ç”¨AIæä¾›å•†æœåŠ¡ï¼ˆä¼˜å…ˆOllamaåˆ†ææ¨¡å‹ï¼‰
      const responseContent = await this.aiProviderService.generateCompletion(
        prompt,
        {
          useAnalysisModel: true,
          systemPrompt,
          temperature: 0.1,
          maxTokens: 1500,
        },
      );

      return this.parseLLMResponse(responseContent, logEntries);
    } catch (error) {
      this.logger.warn(`LLMå¤„ç†å¤±è´¥: ${error.message}`);
      throw error;
    }
  }

  // è§£æLLMå“åº” - ä¼˜åŒ–ä¸ºæ”¯æŒå­—ç¬¦ä¸²å“åº”
  private parseLLMResponse(
    content: string,
    originalLogs: any[],
  ): LLMFeatureExtractionResult[] {
    try {
      // ğŸ”¥ ä¼˜å…ˆå°è¯•è§£æå­—ç¬¦ä¸²å“åº”ï¼Œè€Œéå¼ºåˆ¶è¦æ±‚JSON
      this.logger.debug(`LLMåŸå§‹å“åº”: ${content.substring(0, 200)}...`);
      
      // æ–¹æ³•1: å°è¯•ç›´æ¥è§£æJSON
      let parsed: any;
      try {
        const jsonMatch = content.match(/\[[\s\S]*\]/) || content.match(/\{[\s\S]*\}/);
        if (jsonMatch) {
          parsed = JSON.parse(jsonMatch[0]);
        }
      } catch (jsonError) {
        this.logger.debug(`JSONè§£æå¤±è´¥ï¼Œè½¬ä¸ºå­—ç¬¦ä¸²åˆ†æ: ${jsonError.message}`);
      }

      // æ–¹æ³•2: å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨å­—ç¬¦ä¸²åˆ†æ
      if (!parsed) {
        return this.parseStringResponse(content, originalLogs);
      }

      // æ–¹æ³•3: æˆåŠŸè§£æJSONï¼Œæ ¼å¼åŒ–ç»“æœ
      const results = Array.isArray(parsed) ? parsed : [parsed];
      return results.map((result, index) => ({
        extractedFields: result.extractedFields || {},
        semanticFeatures: {
          errorCategory: result.semanticFeatures?.errorCategory || 'UNKNOWN',
          severity: result.semanticFeatures?.severity || 'MEDIUM',
          businessContext: result.semanticFeatures?.businessContext || [],
          technicalContext: result.semanticFeatures?.technicalContext || [],
          keywords: result.semanticFeatures?.keywords || [],
          sentiment: result.semanticFeatures?.sentiment || 'NEUTRAL',
          urgency: result.semanticFeatures?.urgency || 5,
        },
        normalizedMessage:
          result.normalizedMessage ||
          result.extractedFields?.message ||
          'Unknown',
        confidence: result.confidence || 0.8,
      }));
    } catch (error) {
      this.logger.warn(`è§£æLLMå“åº”å¤±è´¥: ${error.message}`);
      // é™çº§åˆ°è§„åˆ™å¤„ç†
      return this.processWithRules(originalLogs);
    }
  }

  // ğŸ”¥ æ–°å¢ï¼šè§£æå­—ç¬¦ä¸²å“åº”çš„æ–¹æ³•
  private parseStringResponse(
    content: string,
    originalLogs: any[],
  ): LLMFeatureExtractionResult[] {
    this.logger.debug('ä½¿ç”¨å­—ç¬¦ä¸²è§£ææ¨¡å¼å¤„ç†LLMå“åº”');
    
    const results: LLMFeatureExtractionResult[] = [];
    
    // æŒ‰è¡Œåˆ†æï¼Œå°è¯•æå–å…³é”®ä¿¡æ¯
    const lines = content.split('\n').filter(line => line.trim());
    
    for (let i = 0; i < originalLogs.length; i++) {
      const log = originalLogs[i];
      const logStr = typeof log === 'string' ? log : JSON.stringify(log);
      
      // ä»LLMå“åº”ä¸­æå–å¯¹åº”çš„åˆ†æç»“æœ
      const analysis = this.extractAnalysisFromText(content, logStr, i);
      
      // ç»“åˆè§„åˆ™å¼•æ“å’ŒLLMæ–‡æœ¬åˆ†æ
      const ruleResult = this.extractWithRules(log);
      
      // åˆå¹¶ç»“æœï¼ŒLLMåˆ†æä¸ºä¸»ï¼Œè§„åˆ™å¼•æ“è¡¥å……
      results.push({
        extractedFields: {
          ...ruleResult.extractedFields,
          ...analysis.extractedFields,
        },
        semanticFeatures: {
          ...ruleResult.semanticFeatures,
          ...analysis.semanticFeatures,
        },
        normalizedMessage: analysis.normalizedMessage || ruleResult.normalizedMessage,
        confidence: Math.max(analysis.confidence, ruleResult.confidence - 0.1),
      });
    }
    
    return results;
  }

  // ğŸ”¥ æ–°å¢ï¼šä»æ–‡æœ¬ä¸­æå–åˆ†æç»“æœ
  private extractAnalysisFromText(content: string, logStr: string, index: number): Partial<LLMFeatureExtractionResult> {
    const result: Partial<LLMFeatureExtractionResult> = {
      extractedFields: {},
      semanticFeatures: {
        errorCategory: 'UNKNOWN' as const,
        severity: 'MEDIUM' as const,
        businessContext: [],
        technicalContext: [],
        keywords: [],
        sentiment: 'NEUTRAL' as const,
        urgency: 5,
      },
      confidence: 0.6,
    };

    // æå–é”™è¯¯ç±»åˆ«
    if (content.toLowerCase().includes('network') || content.toLowerCase().includes('ç½‘ç»œ')) {
      result.semanticFeatures.errorCategory = 'NETWORK';
    } else if (content.toLowerCase().includes('database') || content.toLowerCase().includes('æ•°æ®åº“')) {
      result.semanticFeatures.errorCategory = 'DATABASE';
    } else if (content.toLowerCase().includes('auth') || content.toLowerCase().includes('ç™»å½•') || content.toLowerCase().includes('è®¤è¯')) {
      result.semanticFeatures.errorCategory = 'AUTHENTICATION';
    } else if (content.toLowerCase().includes('business') || content.toLowerCase().includes('ä¸šåŠ¡')) {
      result.semanticFeatures.errorCategory = 'BUSINESS_LOGIC';
    } else if (content.toLowerCase().includes('system') || content.toLowerCase().includes('ç³»ç»Ÿ')) {
      result.semanticFeatures.errorCategory = 'SYSTEM';
    }

    // æå–ä¸¥é‡ç¨‹åº¦
    if (content.toLowerCase().includes('critical') || content.toLowerCase().includes('ä¸¥é‡') || content.toLowerCase().includes('å…³é”®')) {
      result.semanticFeatures.severity = 'CRITICAL';
    } else if (content.toLowerCase().includes('high') || content.toLowerCase().includes('é«˜')) {
      result.semanticFeatures.severity = 'HIGH';
    } else if (content.toLowerCase().includes('medium') || content.toLowerCase().includes('ä¸­ç­‰')) {
      result.semanticFeatures.severity = 'MEDIUM';
    } else if (content.toLowerCase().includes('low') || content.toLowerCase().includes('ä½')) {
      result.semanticFeatures.severity = 'LOW';
    }

    // æå–å…³é”®è¯
    const keywords = [];
    const keywordPatterns = [
      /é”™è¯¯|error/gi,
      /ç™»å½•|login/gi,
      /ç”¨æˆ·|user/gi,
      /ç½‘ç»œ|network/gi,
      /æ•°æ®åº“|database/gi,
      /è¶…æ—¶|timeout/gi,
      /å¤±è´¥|fail/gi,
      /æˆåŠŸ|success/gi,
    ];

    keywordPatterns.forEach(pattern => {
      const matches = content.match(pattern);
      if (matches) {
        keywords.push(...matches.map(m => m.toLowerCase()));
      }
    });

    result.semanticFeatures.keywords = [...new Set(keywords)];

    // æƒ…æ„Ÿåˆ†æ
    if (content.toLowerCase().includes('error') || content.toLowerCase().includes('é”™è¯¯') || 
        content.toLowerCase().includes('fail') || content.toLowerCase().includes('å¤±è´¥')) {
      result.semanticFeatures.sentiment = 'NEGATIVE';
    } else if (content.toLowerCase().includes('success') || content.toLowerCase().includes('æˆåŠŸ') ||
               content.toLowerCase().includes('ok') || content.toLowerCase().includes('æ­£å¸¸')) {
      result.semanticFeatures.sentiment = 'POSITIVE';
    } else {
      result.semanticFeatures.sentiment = 'NEUTRAL';
    }

    // æå–ç´§æ€¥ç¨‹åº¦
    if (result.semanticFeatures.severity === 'CRITICAL') {
      result.semanticFeatures.urgency = 9;
    } else if (result.semanticFeatures.severity === 'HIGH') {
      result.semanticFeatures.urgency = 7;
    } else if (result.semanticFeatures.severity === 'MEDIUM') {
      result.semanticFeatures.urgency = 5;
    } else {
      result.semanticFeatures.urgency = 3;
    }

    return result;
  }

  // è§„åˆ™å¼•æ“å¤„ç†ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
  private processWithRules(logEntries: any[]): LLMFeatureExtractionResult[] {
    return logEntries.map((log) => this.extractWithRules(log));
  }

  private extractWithRules(log: any): LLMFeatureExtractionResult {
    const logStr = typeof log === 'string' ? log : JSON.stringify(log);

    // åŸºç¡€å­—æ®µæå–
    const extractedFields = {
      timestamp: this.smartExtractField(log, logStr, [
        'timestamp',
        'time',
        'ts',
        'dt',
        'date',
      ]),
      level: this.smartExtractLevel(log, logStr),
      message: this.smartExtractField(log, logStr, [
        'message',
        'msg',
        'desc',
        'text',
        'description',
      ]),
      source: this.smartExtractField(log, logStr, [
        'source',
        'src',
        'logger',
        'component',
      ]),
      service: this.smartExtractField(log, logStr, [
        'service',
        'svc',
        'app',
        'application',
      ]),
      userId: this.smartExtractField(log, logStr, [
        'user',
        'uid',
        'userid',
        'user_id',
      ]),
      sessionId: this.smartExtractField(log, logStr, [
        'session',
        'sess',
        'sessionid',
        'session_id',
      ]),
      responseTime: this.smartExtractNumber(log, logStr, [
        'response_time',
        'duration',
        'elapsed',
        'latency',
      ]),
      errorCode: this.smartExtractField(log, logStr, [
        'error_code',
        'errcode',
        'code',
        'status_code',
      ]),
    };

    // è¯­ä¹‰ç‰¹å¾åˆ†æ
    const message = extractedFields.message || logStr;
    const semanticFeatures = {
      errorCategory: this.categorizeError(message),
      severity: this.analyzeSeverity(extractedFields.level, message),
      businessContext: this.extractContext(message, [
        'order',
        'payment',
        'user',
        'product',
        'cart',
      ]),
      technicalContext: this.extractContext(message, [
        'database',
        'network',
        'api',
        'server',
        'cache',
      ]),
      keywords: this.extractKeywords(message),
      sentiment: this.analyzeSentiment(message),
      urgency: this.calculateUrgency(extractedFields.level, message),
    };

    return {
      extractedFields,
      semanticFeatures,
      normalizedMessage: this.normalizeMessage(message),
      confidence: 0.7, // è§„åˆ™å¼•æ“çš„åŸºç¡€ä¿¡å¿ƒåº¦
    };
  }

  // æ™ºèƒ½å­—æ®µæå–
  private smartExtractField(
    log: any,
    logStr: string,
    fieldNames: string[],
  ): string | undefined {
    // ä»å¯¹è±¡å±æ€§ä¸­æŸ¥æ‰¾
    if (typeof log === 'object' && log !== null) {
      for (const fieldName of fieldNames) {
        if (log[fieldName] !== undefined) {
          return String(log[fieldName]);
        }
      }
    }

    // ä»å­—ç¬¦ä¸²ä¸­æ­£åˆ™åŒ¹é…
    for (const fieldName of fieldNames) {
      const patterns = [
        new RegExp(`${fieldName}[:\\s=]+([^\\s,}]+)`, 'i'),
        new RegExp(`"${fieldName}"[:\\s]*"([^"]+)"`, 'i'),
        new RegExp(`${fieldName}\\s*=\\s*([^\\s,]+)`, 'i'),
      ];

      for (const pattern of patterns) {
        const match = logStr.match(pattern);
        if (match && match[1]) {
          return match[1].trim();
        }
      }
    }

    return undefined;
  }

  // æ™ºèƒ½æ—¥å¿—çº§åˆ«æå–
  private smartExtractLevel(log: any, logStr: string): string | undefined {
    const levelPatterns = [
      /level[:\s=]+(\w+)/i,
      /\b(DEBUG|INFO|WARN|ERROR|FATAL)\b/i,
      /\[(DEBUG|INFO|WARN|ERROR|FATAL)\]/i,
    ];

    for (const pattern of levelPatterns) {
      const match = logStr.match(pattern);
      if (match && match[1]) {
        return match[1].toUpperCase();
      }
    }

    return undefined;
  }

  // æ™ºèƒ½æ•°å­—å­—æ®µæå–
  private smartExtractNumber(
    log: any,
    logStr: string,
    fieldNames: string[],
  ): number | undefined {
    const value = this.smartExtractField(log, logStr, fieldNames);
    if (value) {
      const num = parseFloat(value);
      return isNaN(num) ? undefined : num;
    }
    return undefined;
  }

  // é”™è¯¯åˆ†ç±»
  private categorizeError(message: string): any {
    const lowerMessage = message.toLowerCase();
    if (lowerMessage.includes('network') || lowerMessage.includes('connection'))
      return 'NETWORK';
    if (lowerMessage.includes('database') || lowerMessage.includes('sql'))
      return 'DATABASE';
    if (lowerMessage.includes('auth') || lowerMessage.includes('permission'))
      return 'AUTHENTICATION';
    if (lowerMessage.includes('business') || lowerMessage.includes('validation'))
      return 'BUSINESS_LOGIC';
    if (lowerMessage.includes('system') || lowerMessage.includes('memory'))
      return 'SYSTEM';
    return 'UNKNOWN';
  }

  // ä¸¥é‡æ€§åˆ†æ
  private analyzeSeverity(level?: string, message?: string): any {
    if (level === 'FATAL' || level === 'ERROR') return 'HIGH';
    if (level === 'WARN') return 'MEDIUM';
    if (message?.toLowerCase().includes('critical')) return 'CRITICAL';
    return 'LOW';
  }

  // ä¸Šä¸‹æ–‡æå–
  private extractContext(message: string, terms: string[]): string[] {
    return terms.filter((term) =>
      message.toLowerCase().includes(term.toLowerCase()),
    );
  }

  // å…³é”®è¯æå–
  private extractKeywords(message: string): string[] {
    const words = message
      .toLowerCase()
      .split(/\W+/)
      .filter((word) => word.length > 3);
    const importantWords = words.filter(
      (word) =>
        !['the', 'and', 'but', 'for', 'with', 'this', 'that'].includes(word),
    );
    return [...new Set(importantWords)].slice(0, 10);
  }

  // æƒ…æ„Ÿåˆ†æ
  private analyzeSentiment(message: string): any {
    const lowerMessage = message.toLowerCase();
    const negativeWords = ['error', 'fail', 'problem', 'issue', 'crash'];
    const positiveWords = ['success', 'complete', 'ok', 'good', 'working'];

    const negativeCount = negativeWords.filter((word) =>
      lowerMessage.includes(word),
    ).length;
    const positiveCount = positiveWords.filter((word) =>
      lowerMessage.includes(word),
    ).length;

    if (negativeCount > positiveCount) return 'NEGATIVE';
    if (positiveCount > negativeCount) return 'POSITIVE';
    return 'NEUTRAL';
  }

  // ç´§æ€¥ç¨‹åº¦è®¡ç®—
  private calculateUrgency(level?: string, message?: string): number {
    let urgency = 5; // åŸºç¡€å€¼
    if (level === 'FATAL') urgency += 4;
    else if (level === 'ERROR') urgency += 3;
    else if (level === 'WARN') urgency += 1;

    if (message?.toLowerCase().includes('critical')) urgency += 2;
    if (message?.toLowerCase().includes('urgent')) urgency += 2;

    return Math.min(urgency, 10);
  }

  // æ¶ˆæ¯æ ‡å‡†åŒ–
  private normalizeMessage(message: string): string {
    return message
      .replace(/\s+/g, ' ')
      .replace(/[^\w\s\-_.]/g, '')
      .trim()
      .substring(0, 200);
  }

  // èšåˆç‰¹å¾
  private aggregateFeatures(results: LLMFeatureExtractionResult[]): any {
    const errorCategories = results.map((r) => r.semanticFeatures.errorCategory);
    const severities = results.map((r) => r.semanticFeatures.severity);

    return {
      totalLogs: results.length,
      errorCategoryDistribution: this.getDistribution(errorCategories),
      severityDistribution: this.getDistribution(severities),
      averageConfidence:
        results.reduce((sum, r) => sum + r.confidence, 0) / results.length,
    };
  }

  private getDistribution(values: string[]): Record<string, number> {
    return values.reduce((acc, value) => {
      acc[value] = (acc[value] || 0) + 1;
      return acc;
    }, {});
  }

  // è®¡ç®—æ•´ä½“ä¿¡å¿ƒåº¦
  private calculateOverallConfidence(
    results: LLMFeatureExtractionResult[],
  ): number {
    if (results.length === 0) return 0;
    return (
      results.reduce((sum, result) => sum + result.confidence, 0) /
      results.length
    );
  }

  // å¤‡ç”¨æå–æ–¹æ¡ˆ
  private async fallbackExtraction(logData: any[]): Promise<any> {
    return {
      message: 'ä½¿ç”¨è§„åˆ™å¼•æ“è¿›è¡ŒåŸºç¡€ç‰¹å¾æå–',
      results: this.processWithRules(logData.slice(0, 10)),
      totalProcessed: Math.min(logData.length, 10),
    };
  }

  // ä¿®å¤åˆ†å±‚ç­–ç•¥æ–¹æ³•
  private buildLLMPromptStratified(logEntries: any[]): string {
    const tiers: LogTier[] = [
      {
        priority: 1,
        logs: [],
        batchSize: this.batchConfig.maxBatchSize,
        description: 'å…³é”®é”™è¯¯å’Œå¼‚å¸¸ (CRITICAL/ERROR)',
      },
      {
        priority: 2,
        logs: [],
        batchSize: this.batchConfig.baseBatchSize * 2,
        description: 'è­¦å‘Šå’Œé‡è¦ä¿¡æ¯ (WARN/é‡è¦INFO)',
      },
      {
        priority: 3,
        logs: [],
        batchSize: this.batchConfig.baseBatchSize * 3,
        description: 'ä¸€èˆ¬ä¿¡æ¯ (INFO/DEBUG/TRACE)',
      },
    ];

    // åˆ†ç±»æ—¥å¿—
    logEntries.forEach(log => {
      const logStr = typeof log === 'string' ? log : JSON.stringify(log);
      const level = this.extractLogLevel(log, logStr).toUpperCase();
      
      if (level.includes('ERROR') || level.includes('CRITICAL') || level.includes('SEVERE')) {
        tiers[0].logs.push(log);
      } else if (level.includes('WARN') || this.isImportantInfo(logStr)) {
        tiers[1].logs.push(log);
      } else {
        tiers[2].logs.push(log);
      }
    });

    // å¯¹å¤§é‡æ—¥å¿—è¿›è¡Œæ™ºèƒ½é‡‡æ ·
    if (tiers[2].logs.length > 500 && this.batchConfig.intelligentSampling) {
      tiers[2].logs = this.intelligentSample(tiers[2].logs, 200);
    }
    
    return this.buildLLMPrompt(logEntries);
  }
} 