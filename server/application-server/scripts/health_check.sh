#!/bin/bash

# ğŸ” Ollama AIæœåŠ¡å¥åº·æ£€æŸ¥è„šæœ¬

echo "ğŸ” å¼€å§‹AIæœåŠ¡å¥åº·æ£€æŸ¥..."
echo "=========================================="

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# æ£€æŸ¥ç»“æœç»Ÿè®¡
passed=0
failed=0

# æ£€æŸ¥å‡½æ•°
check_service() {
    local service_name=$1
    local check_command=$2
    local description=$3
    
    echo -n "æ£€æŸ¥ $service_name: "
    
    if eval "$check_command" &> /dev/null; then
        echo -e "${GREEN}âœ… æ­£å¸¸${NC} - $description"
        ((passed++))
        return 0
    else
        echo -e "${RED}âŒ å¼‚å¸¸${NC} - $description"
        ((failed++))
        return 1
    fi
}

# 1. æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
echo "1. ğŸš€ OllamaæœåŠ¡æ£€æŸ¥"
echo "------------------------------------------"

check_service "Ollamaè¿›ç¨‹" "pgrep ollama" "Ollamaå®ˆæŠ¤è¿›ç¨‹æ­£åœ¨è¿è¡Œ"
check_service "Ollama API" "curl -s http://localhost:11434/api/tags" "APIæ¥å£å“åº”æ­£å¸¸"

# 2. æ£€æŸ¥å¿…éœ€æ¨¡å‹
echo ""
echo "2. ğŸ¤– AIæ¨¡å‹æ£€æŸ¥"
echo "------------------------------------------"

REQUIRED_MODELS=(
    "qwen2.5:14b:ä¸»è¦å¯¹è¯æ¨¡å‹"
    "qwen2.5:7b:å¿«é€Ÿå“åº”æ¨¡å‹"
    "deepseek-coder:6.7b:æŠ€æœ¯åˆ†ææ¨¡å‹"
    "nomic-embed-text:å‘é‡åµŒå…¥æ¨¡å‹"
)

for model_info in "${REQUIRED_MODELS[@]}"; do
    IFS=':' read -r model_name model_desc <<< "$model_info"
    check_service "$model_name" "ollama list | grep -q '$model_name'" "$model_desc"
done

# 3. æ£€æŸ¥æ¨¡å‹åŠŸèƒ½
echo ""
echo "3. ğŸ§ª æ¨¡å‹åŠŸèƒ½æµ‹è¯•"
echo "------------------------------------------"

# æµ‹è¯•å¯¹è¯æ¨¡å‹
if ollama list | grep -q "qwen2.5:14b"; then
    echo -n "æµ‹è¯•å¯¹è¯æ¨¡å‹å“åº”: "
    if timeout 30s bash -c 'echo "ä½ å¥½" | ollama run qwen2.5:14b' &> /dev/null; then
        echo -e "${GREEN}âœ… æ­£å¸¸${NC} - æ¨¡å‹å¯ä»¥æ­£å¸¸å¯¹è¯"
        ((passed++))
    else
        echo -e "${YELLOW}âš ï¸  è¶…æ—¶${NC} - æ¨¡å‹å“åº”è¾ƒæ…¢æˆ–æœ‰é—®é¢˜"
        ((failed++))
    fi
else
    echo -e "${RED}âŒ è·³è¿‡${NC} - å¯¹è¯æ¨¡å‹æœªå®‰è£…"
    ((failed++))
fi

# æµ‹è¯•åµŒå…¥æ¨¡å‹
if ollama list | grep -q "nomic-embed-text"; then
    echo -n "æµ‹è¯•åµŒå…¥æ¨¡å‹API: "
    if curl -s -X POST http://localhost:11434/api/embeddings \
        -H "Content-Type: application/json" \
        -d '{"model": "nomic-embed-text", "prompt": "test"}' | grep -q "embedding"; then
        echo -e "${GREEN}âœ… æ­£å¸¸${NC} - å‘é‡åŒ–åŠŸèƒ½æ­£å¸¸"
        ((passed++))
    else
        echo -e "${RED}âŒ å¼‚å¸¸${NC} - å‘é‡åŒ–APIå“åº”å¼‚å¸¸"
        ((failed++))
    fi
else
    echo -e "${RED}âŒ è·³è¿‡${NC} - åµŒå…¥æ¨¡å‹æœªå®‰è£…"
    ((failed++))
fi

# 4. æ£€æŸ¥ç³»ç»Ÿèµ„æº
echo ""
echo "4. ğŸ’» ç³»ç»Ÿèµ„æºæ£€æŸ¥"
echo "------------------------------------------"

# å†…å­˜æ£€æŸ¥
total_mem=$(free -h | awk '/^Mem:/ {print $2}' 2>/dev/null || echo "æœªçŸ¥")
available_mem=$(free -h | awk '/^Mem:/ {print $7}' 2>/dev/null || echo "æœªçŸ¥")
echo "ç³»ç»Ÿå†…å­˜: æ€»è®¡ $total_mem, å¯ç”¨ $available_mem"

# ç£ç›˜ç©ºé—´æ£€æŸ¥
ollama_dir="$HOME/.ollama"
if [ -d "$ollama_dir" ]; then
    disk_usage=$(du -sh "$ollama_dir" 2>/dev/null | cut -f1)
    echo "Ollamaå­˜å‚¨: $disk_usage (è·¯å¾„: $ollama_dir)"
else
    echo "Ollamaå­˜å‚¨: æœªæ‰¾åˆ°æ•°æ®ç›®å½•"
fi

# CPUè´Ÿè½½æ£€æŸ¥
if command -v uptime &> /dev/null; then
    load_avg=$(uptime | awk -F'load average:' '{print $2}')
    echo "CPUè´Ÿè½½: $load_avg"
fi

# 5. æ£€æŸ¥é¡¹ç›®æœåŠ¡
echo ""
echo "5. ğŸŒ é¡¹ç›®æœåŠ¡æ£€æŸ¥"
echo "------------------------------------------"

# æ£€æŸ¥é¡¹ç›®æ˜¯å¦è¿è¡Œ
if curl -s http://localhost:3000/api/ai/health &> /dev/null; then
    echo -e "${GREEN}âœ… é¡¹ç›®æœåŠ¡æ­£å¸¸${NC} - AI APIå¯è®¿é—®"
    ((passed++))
else
    echo -e "${YELLOW}âš ï¸  é¡¹ç›®æœåŠ¡æœªè¿è¡Œ${NC} - è¯·å¯åŠ¨: pnpm run start:dev"
fi

# æ£€æŸ¥Qdrantå‘é‡æ•°æ®åº“
if curl -s http://localhost:6333/health &> /dev/null; then
    echo -e "${GREEN}âœ… Qdrantæ­£å¸¸${NC} - å‘é‡æ•°æ®åº“å¯è®¿é—®"
    ((passed++))
else
    echo -e "${YELLOW}âš ï¸  Qdrantæœªè¿è¡Œ${NC} - è¯·å¯åŠ¨: docker-compose up -d qdrant"
fi

# 6. æ€§èƒ½å»ºè®®
echo ""
echo "6. ğŸ’¡ æ€§èƒ½å»ºè®®"
echo "------------------------------------------"

# æ£€æŸ¥æ˜¯å¦æœ‰GPU
if command -v nvidia-smi &> /dev/null; then
    echo "ğŸ® æ£€æµ‹åˆ°NVIDIA GPUï¼Œå»ºè®®å¯ç”¨GPUåŠ é€Ÿ"
    echo "   è®¾ç½®ç¯å¢ƒå˜é‡: export CUDA_VISIBLE_DEVICES=0"
else
    echo "âš¡ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUæ¨ç†"
    echo "   å»ºè®®: å¢åŠ CPUæ ¸å¿ƒæ•°æˆ–ä½¿ç”¨è¾ƒå°æ¨¡å‹"
fi

# å†…å­˜å»ºè®®
available_mb=$(free -m | awk '/^Mem:/ {print $7}' 2>/dev/null || echo "0")
if [ "$available_mb" -lt 4096 ]; then
    echo "âš ï¸  å¯ç”¨å†…å­˜è¾ƒå°‘ (<4GB)ï¼Œå»ºè®®:"
    echo "   - ä½¿ç”¨è½»é‡çº§æ¨¡å‹: qwen2.5:1.5b"
    echo "   - è®¾ç½®: export OLLAMA_MAX_LOADED_MODELS=1"
fi

# æ€»ç»“
echo ""
echo "=========================================="
echo "ğŸ å¥åº·æ£€æŸ¥å®Œæˆ"
echo "=========================================="
echo -e "é€šè¿‡: ${GREEN}$passed${NC} é¡¹"
echo -e "å¤±è´¥: ${RED}$failed${NC} é¡¹"

if [ $failed -eq 0 ]; then
    echo -e "${GREEN}ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸${NC}"
    exit 0
elif [ $failed -le 2 ]; then
    echo -e "${YELLOW}âš ï¸  å‘ç°è½»å¾®é—®é¢˜ï¼Œç³»ç»ŸåŸºæœ¬å¯ç”¨${NC}"
    exit 1
else
    echo -e "${RED}âŒ å‘ç°ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ä¿®å¤åæ‰èƒ½æ­£å¸¸ä½¿ç”¨${NC}"
    echo ""
    echo "ğŸ”§ å¸¸è§ä¿®å¤æ–¹æ³•ï¼š"
    echo "1. å¯åŠ¨Ollama: ollama serve"
    echo "2. ä¸‹è½½æ¨¡å‹: ./scripts/download_models.sh"
    echo "3. å¯åŠ¨é¡¹ç›®: pnpm run start:dev"
    echo "4. å¯åŠ¨å‘é‡æ•°æ®åº“: docker-compose up -d qdrant"
    exit 2
fi 