# åŸºäºLangChainçš„é€šç”¨AIæœåŠ¡æŒ‡å—

## æ¦‚è¿°

æœ¬é¡¹ç›®é‡‡ç”¨LangChainæ¡†æ¶å®ç°äº†é€šç”¨çš„AIæœåŠ¡ï¼Œæ”¯æŒå¤šç§æ¨¡å‹æä¾›å•†ï¼ˆOllamaã€OpenAIã€è‡ªå®šä¹‰APIï¼‰ï¼Œå…·å¤‡è‡ªåŠ¨é™çº§ã€æ™ºèƒ½è·¯ç”±ç­‰é«˜çº§åŠŸèƒ½ã€‚

## æ¶æ„è®¾è®¡

### æ ¸å¿ƒç»„ä»¶

1. **UniversalAIService** - åº•å±‚AIæœåŠ¡ï¼ŒåŸºäºLangChainç»Ÿä¸€æ¥å£
2. **LangChainAIProviderService** - é«˜çº§AIæœåŠ¡æä¾›å•†ï¼Œæ”¯æŒè‡ªåŠ¨é™çº§
3. **UniversalAIController** - RESTful APIæ§åˆ¶å™¨

### æ”¯æŒçš„æ¨¡å‹æä¾›å•†

- **Ollama** - æœ¬åœ°æ¨¡å‹ï¼ˆä¸»è¦æ¨èï¼‰
  - ä¼˜åŠ¿ï¼šéšç§ä¿æŠ¤ã€é›¶æˆæœ¬ã€å¯å®šåˆ¶
  - æ”¯æŒæ¨¡å‹ï¼šqwen2.5:14b, qwen2.5:7b, deepseek-coder:6.7b ç­‰

- **OpenAI** - äº‘ç«¯æ¨¡å‹
  - ä¼˜åŠ¿ï¼šé«˜è´¨é‡ã€ç¨³å®šæ€§å¥½
  - æ”¯æŒæ¨¡å‹ï¼šgpt-4o, gpt-4-turbo, gpt-3.5-turbo ç­‰

- **Custom API** - è‡ªå®šä¹‰APIï¼ˆå…¼å®¹OpenAIæ ¼å¼ï¼‰
  - ä¼˜åŠ¿ï¼šçµæ´»æ€§é«˜ã€æ”¯æŒç¬¬ä¸‰æ–¹æœåŠ¡
  - é€‚ç”¨äºï¼šMoonshotã€æ™ºè°±AIã€ç™¾å·AIç­‰

## é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# AIæœåŠ¡æä¾›å•†é€‰æ‹©
AI_PROVIDER=ollama  # ollama | openai | custom-api

# Ollamaé…ç½®ï¼ˆæ¨èï¼‰
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_CHAT_MODEL=qwen2.5:14b
OLLAMA_FAST_MODEL=qwen2.5:7b
OLLAMA_ANALYSIS_MODEL=deepseek-coder:6.7b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# OpenAIé…ç½®
OPENAI_API_KEY=your_openai_key
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-3.5-turbo

# è‡ªå®šä¹‰APIé…ç½®
CUSTOM_API_KEY=your_custom_key
CUSTOM_BASE_URL=https://api.example.com/v1
CUSTOM_MODEL=custom-model
CUSTOM_MODELS=model1,model2,model3  # å¯ç”¨æ¨¡å‹åˆ—è¡¨

# é€šç”¨é…ç½®
AI_TEMPERATURE=0.7
AI_MAX_TOKENS=2000
```

### é…ç½®æ–‡ä»¶ (src/config/configuration.ts)

é…ç½®æ–‡ä»¶å·²è‡ªåŠ¨è¯»å–ç¯å¢ƒå˜é‡ï¼Œæ— éœ€æ‰‹åŠ¨ä¿®æ”¹ã€‚

## API ä½¿ç”¨æŒ‡å—

### 1. åŸºç¡€å¯¹è¯

```bash
# æ™®é€šå¯¹è¯
curl -X POST http://localhost:3000/ai/universal/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}
    ],
    "temperature": 0.7,
    "maxTokens": 1000
  }'

# æŒ‡å®šæ¨¡å‹æä¾›å•†
curl -X POST http://localhost:3000/ai/universal/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "}
    ],
    "provider": "ollama",
    "useAnalysisModel": true,
    "temperature": 0.3
  }'
```

### 2. æ–‡æœ¬ç”Ÿæˆ

```bash
# åŸºç¡€æ–‡æœ¬ç”Ÿæˆ
curl -X POST http://localhost:3000/ai/universal/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„ç®€çŸ­æ–‡ç« ",
    "systemPrompt": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç§‘æŠ€ä½œå®¶",
    "temperature": 0.8,
    "maxTokens": 2000
  }'

# å¿«é€Ÿæ¨¡å¼
curl -X POST http://localhost:3000/ai/universal/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "æ€»ç»“ä»Šå¤©çš„æ–°é—»è¦ç‚¹",
    "fastMode": true,
    "maxTokens": 500
  }'
```

### 3. æ‰¹é‡å¤„ç†

```bash
curl -X POST http://localhost:3000/ai/universal/batch \
  -H "Content-Type: application/json" \
  -d '{
    "prompts": [
      "ç¿»è¯‘ï¼šHello World",
      "è§£é‡Šï¼šä»€ä¹ˆæ˜¯REST API",
      "æ€»ç»“ï¼šäººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹"
    ],
    "temperature": 0.5,
    "maxTokens": 800
  }'
```

### 4. æ™ºèƒ½å¯¹è¯æ¼”ç¤º

```bash
# ä»£ç å®¡æŸ¥åœºæ™¯
curl -X POST http://localhost:3000/ai/universal/demo/smart-chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "function add(a, b) { return a + b }",
    "scenario": "code-review"
  }'

# åˆ›æ„å†™ä½œåœºæ™¯
curl -X POST http://localhost:3000/ai/universal/demo/smart-chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "å†™ä¸€ä¸ªå…³äºæœªæ¥åŸå¸‚çš„ç§‘å¹»æ•…äº‹å¼€å¤´",
    "scenario": "creative-writing"
  }'

# å¿«é€Ÿé—®ç­”åœºæ™¯
curl -X POST http://localhost:3000/ai/universal/demo/smart-chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "ä»€ä¹ˆæ˜¯TCP/IPåè®®ï¼Ÿ",
    "scenario": "quick-answer"
  }'
```

### 5. ç³»ç»Ÿç®¡ç†

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:3000/ai/universal/health

# è·å–å¯ç”¨æ¨¡å‹
curl http://localhost:3000/ai/universal/models
curl "http://localhost:3000/ai/universal/models?provider=ollama"

# åˆ‡æ¢æ¨¡å‹
curl -X POST http://localhost:3000/ai/universal/switch-model \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "ollama",
    "modelName": "qwen2.5:7b"
  }'

# è·å–æ¨èé…ç½®
curl "http://localhost:3000/ai/universal/recommended-config?scenario=analysis"

# è·å–ç»Ÿè®¡ä¿¡æ¯
curl http://localhost:3000/ai/universal/stats
```

## ä»£ç ä½¿ç”¨ç¤ºä¾‹

### åœ¨Serviceä¸­ä½¿ç”¨

```typescript
import { Injectable } from '@nestjs/common';
import { LangChainAIProviderService } from '../ai/services/langchain-ai-provider.service';

@Injectable()
export class MyService {
  constructor(
    private readonly aiProvider: LangChainAIProviderService,
  ) {}

  // åŸºç¡€å¯¹è¯
  async basicChat(userMessage: string): Promise<string> {
    const response = await this.aiProvider.chat([
      { role: 'user', content: userMessage }
    ]);
    return response.content;
  }

  // ä»£ç åˆ†æ
  async analyzeCode(code: string): Promise<string> {
    return await this.aiProvider.generateCompletion(
      `è¯·åˆ†æè¿™æ®µä»£ç çš„è´¨é‡å’Œæ€§èƒ½ï¼š\n${code}`,
      {
        useAnalysisModel: true,
        temperature: 0.3,
        systemPrompt: 'ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä»£ç å®¡æŸ¥ä¸“å®¶',
      }
    );
  }

  // å¿«é€Ÿå›ç­”
  async quickAnswer(question: string): Promise<string> {
    return await this.aiProvider.generateCompletion(question, {
      fastMode: true,
      temperature: 0.5,
      maxTokens: 500,
    });
  }

  // æ‰¹é‡ç¿»è¯‘
  async batchTranslate(texts: string[]): Promise<string[]> {
    const prompts = texts.map(text => `ç¿»è¯‘æˆè‹±æ–‡ï¼š${text}`);
    return await this.aiProvider.batchProcess(prompts, {
      temperature: 0.3,
      maxTokens: 200,
    });
  }

  // æ™ºèƒ½åœºæ™¯é€‰æ‹©
  async smartProcess(content: string, type: 'code' | 'creative' | 'analysis'): Promise<string> {
    const configs = {
      code: {
        useAnalysisModel: true,
        temperature: 0.3,
        systemPrompt: 'ä½ æ˜¯ä»£ç åˆ†æä¸“å®¶',
      },
      creative: {
        temperature: 0.9,
        maxTokens: 3000,
        systemPrompt: 'ä½ æ˜¯åˆ›æ„å†™ä½œä¸“å®¶',
      },
      analysis: {
        useAnalysisModel: true,
        temperature: 0.1,
        maxTokens: 4000,
        systemPrompt: 'ä½ æ˜¯æ•°æ®åˆ†æä¸“å®¶',
      },
    };

    return await this.aiProvider.generateCompletion(content, configs[type]);
  }
}
```

### è‡ªåŠ¨é™çº§ç¤ºä¾‹

```typescript
// ç³»ç»Ÿä¼šè‡ªåŠ¨å°è¯•ä»¥ä¸‹é¡ºåºï¼š
// 1. ä¸»è¦providerï¼ˆä»é…ç½®è·å–ï¼Œé»˜è®¤ollamaï¼‰
// 2. ç¬¬äºŒé€‰æ‹©ï¼ˆopenaiï¼‰
// 3. ç¬¬ä¸‰é€‰æ‹©ï¼ˆcustom-apiï¼‰

const response = await this.aiProvider.chat(messages, {
  // ä¸æŒ‡å®šproviderï¼Œç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©å¹¶é™çº§
  temperature: 0.7,
});

// ä¹Ÿå¯ä»¥æ‰‹åŠ¨æŒ‡å®šé™çº§é¡ºåº
const response = await this.aiProvider.chat(messages, {
  provider: 'ollama',
  fallbackProviders: ['openai', 'custom-api'],
});
```

## é«˜çº§ç‰¹æ€§

### 1. æ™ºèƒ½æ¨¡å‹é€‰æ‹©

ç³»ç»Ÿæ ¹æ®ä¸åŒåœºæ™¯è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹ï¼š

- **å¯¹è¯åœºæ™¯**ï¼šå‡è¡¡çš„å¯¹è¯æ¨¡å‹
- **åˆ†æåœºæ™¯**ï¼šä¸“é—¨çš„åˆ†ææ¨¡å‹ï¼ˆå¦‚deepseek-coderï¼‰
- **å¿«é€Ÿåœºæ™¯**ï¼šè½»é‡çº§æ¨¡å‹
- **åˆ›æ„åœºæ™¯**ï¼šé«˜æ¸©åº¦è®¾ç½®çš„åˆ›æ„æ¨¡å‹

### 2. è‡ªåŠ¨é™çº§æœºåˆ¶

å½“ä¸»è¦æœåŠ¡ä¸å¯ç”¨æ—¶ï¼Œç³»ç»Ÿè‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æœåŠ¡ï¼š

```
Ollama (æœ¬åœ°) â†’ OpenAI (äº‘ç«¯) â†’ Custom API (å¤‡ç”¨)
```

### 3. æ¨¡å‹ç¼“å­˜

ç³»ç»Ÿç¼“å­˜å·²åˆå§‹åŒ–çš„æ¨¡å‹å®ä¾‹ï¼Œæé«˜å“åº”é€Ÿåº¦ã€‚

### 4. å¥åº·ç›‘æ§

å®šæœŸæ£€æŸ¥æ‰€æœ‰AIæœåŠ¡çš„å¥åº·çŠ¶æ€ï¼Œæä¾›è¯¦ç»†çš„ç›‘æ§ä¿¡æ¯ã€‚

## éƒ¨ç½²å»ºè®®

### 1. æœ¬åœ°å¼€å‘ç¯å¢ƒ

```bash
# å®‰è£…Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# ä¸‹è½½æ¨èæ¨¡å‹
ollama pull qwen2.5:14b
ollama pull qwen2.5:7b
ollama pull deepseek-coder:6.7b
ollama pull nomic-embed-text

# è®¾ç½®ç¯å¢ƒå˜é‡
export AI_PROVIDER=ollama
export OLLAMA_BASE_URL=http://localhost:11434

# å¯åŠ¨é¡¹ç›®
pnpm run start:dev
```

### 2. ç”Ÿäº§ç¯å¢ƒ

```bash
# 1. é…ç½®ç¯å¢ƒå˜é‡
export AI_PROVIDER=ollama
export OLLAMA_BASE_URL=http://ollama-server:11434
export OPENAI_API_KEY=backup_key  # ä½œä¸ºå¤‡ç”¨

# 2. éƒ¨ç½²OllamaæœåŠ¡å™¨
docker run -d \
  --name ollama \
  -p 11434:11434 \
  -v ollama:/root/.ollama \
  ollama/ollama

# 3. å¯åŠ¨åº”ç”¨
pnpm run build
pnpm run start:prod
```

### 3. äº‘ç«¯éƒ¨ç½²

å¦‚æœéœ€è¦äº‘ç«¯æœåŠ¡ä½œä¸ºä¸»è¦providerï¼š

```bash
export AI_PROVIDER=openai
export OPENAI_API_KEY=your_openai_key
export CUSTOM_BASE_URL=http://backup-api:8080  # å¤‡ç”¨API
```

## æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥

- **èŠå¤©**ï¼šä½¿ç”¨ä¸­ç­‰å¤§å°æ¨¡å‹ (qwen2.5:14b)
- **å¿«é€Ÿå“åº”**ï¼šä½¿ç”¨å°æ¨¡å‹ (qwen2.5:7b)
- **æ·±åº¦åˆ†æ**ï¼šä½¿ç”¨ä¸“ä¸šæ¨¡å‹ (deepseek-coder:6.7b)

### 2. å¹¶å‘æ§åˆ¶

```typescript
// æ‰¹é‡å¤„ç†è‡ªåŠ¨é™åˆ¶å¹¶å‘æ•°
const results = await this.aiProvider.batchProcess(prompts, {
  maxConcurrency: 3,  // ç³»ç»Ÿè‡ªåŠ¨ç®¡ç†
});
```

### 3. ç¼“å­˜ä¼˜åŒ–

- æ¨¡å‹å®ä¾‹ç¼“å­˜
- é…ç½®ç¼“å­˜
- ç»“æœç¼“å­˜ï¼ˆå¯é€‰ï¼‰

## æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

1. **Ollamaè¿æ¥å¤±è´¥**
   ```bash
   # æ£€æŸ¥OllamaçŠ¶æ€
   curl http://localhost:11434/api/tags
   
   # é‡å¯Ollama
   ollama serve
   ```

2. **æ¨¡å‹ä¸å­˜åœ¨**
   ```bash
   # æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹
   ollama list
   
   # ä¸‹è½½æ¨¡å‹
   ollama pull qwen2.5:14b
   ```

3. **APIå¯†é’¥æ— æ•ˆ**
   ```bash
   # æ£€æŸ¥ç¯å¢ƒå˜é‡
   echo $OPENAI_API_KEY
   echo $CUSTOM_API_KEY
   ```

### è°ƒè¯•æŠ€å·§

```bash
# æŸ¥çœ‹å¥åº·çŠ¶æ€
curl http://localhost:3000/ai/universal/health

# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
curl http://localhost:3000/ai/universal/stats

# æµ‹è¯•æ¨¡å‹åˆ‡æ¢
curl -X POST http://localhost:3000/ai/universal/switch-model \
  -H "Content-Type: application/json" \
  -d '{"provider": "ollama", "modelName": "qwen2.5:7b"}'
```

## æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©

- å¼€å‘ç¯å¢ƒï¼šä¼˜å…ˆä½¿ç”¨Ollama
- ç”Ÿäº§ç¯å¢ƒï¼šOllama + OpenAIå¤‡ç”¨
- é«˜è´Ÿè½½åœºæ™¯ï¼šä½¿ç”¨fastMode

### 2. æç¤ºè¯ä¼˜åŒ–

```typescript
// å¥½çš„åšæ³•
const systemPrompt = 'ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œä¸“æ³¨äºä»£ç è´¨é‡å’Œæœ€ä½³å®è·µã€‚';
const userPrompt = 'è¯·å®¡æŸ¥ä»¥ä¸‹TypeScriptä»£ç å¹¶æå‡ºå…·ä½“çš„æ”¹è¿›å»ºè®®ï¼š\n\n' + code;

// é¿å…çš„åšæ³•
const prompt = 'çœ‹çœ‹è¿™ä¸ªä»£ç æœ‰ä»€ä¹ˆé—®é¢˜ï¼š' + code;
```

### 3. é”™è¯¯å¤„ç†

```typescript
try {
  const response = await this.aiProvider.chat(messages);
  return response.content;
} catch (error) {
  // ç³»ç»Ÿå·²ç»è‡ªåŠ¨é™çº§ï¼Œè¿™é‡Œæ˜¯æœ€åçš„é”™è¯¯
  this.logger.error('æ‰€æœ‰AIæœåŠ¡éƒ½ä¸å¯ç”¨', error);
  return 'æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚';
}
```

### 4. ç›‘æ§å’Œæ—¥å¿—

```typescript
// è®°å½•ä½¿ç”¨ç»Ÿè®¡
const startTime = Date.now();
const response = await this.aiProvider.chat(messages);
this.logger.log(`AIè°ƒç”¨å®Œæˆ`, {
  provider: response.provider,
  model: response.model,
  processingTime: response.processingTime,
  inputTokens: response.usage?.promptTokens,
  outputTokens: response.usage?.completionTokens,
});
```

## æ›´æ–°æ—¥å¿—

### v2.0.0 - LangChainé›†æˆ
- âœ… é›†æˆLangChainæ¡†æ¶
- âœ… æ”¯æŒå¤šç§æ¨¡å‹æä¾›å•†
- âœ… è‡ªåŠ¨é™çº§æœºåˆ¶
- âœ… æ™ºèƒ½æ¨¡å‹é€‰æ‹©
- âœ… ç»Ÿä¸€APIæ¥å£
- âœ… å¥åº·ç›‘æ§
- âœ… æ‰¹é‡å¤„ç†ä¼˜åŒ–

### åç»­è®¡åˆ’
- ğŸš§ æµå¼å“åº”æ”¯æŒ
- ğŸš§ ç»“æœç¼“å­˜ä¼˜åŒ–
- ğŸš§ æ›´å¤šæ¨¡å‹æä¾›å•†æ”¯æŒ
- ğŸš§ æ€§èƒ½ç›‘æ§é¢æ¿
- ğŸš§ A/Bæµ‹è¯•åŠŸèƒ½

---

é€šè¿‡è¿™ä¸ªé€šç”¨çš„AIæœåŠ¡æ¶æ„ï¼Œä½ å¯ä»¥è½»æ¾åœ°åœ¨ä¸åŒçš„AIæ¨¡å‹æä¾›å•†ä¹‹é—´åˆ‡æ¢ï¼Œäº«å—æœ¬åœ°æ¨¡å‹çš„éšç§ä¼˜åŠ¿å’Œäº‘ç«¯æ¨¡å‹çš„é«˜è´¨é‡æœåŠ¡ï¼ŒåŒæ—¶å…·å¤‡å®Œå–„çš„é™çº§å’Œç›‘æ§æœºåˆ¶ã€‚ 